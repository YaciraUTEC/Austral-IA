
from austral.gpt_azure import chat_completion, extraer_objeto_gpt
from austral.search_service import buscar_fragmentos
import difflib

# Memoria para guardar el contexto de la conversaci√≥n por usuario
HISTORIAL_CONVERSACION = {}
MAX_MENSAJES_HISTORIAL = 5

SYSTEM_PROMPT= f"""
   "Eres Austral IA, un asistente t√©cnico confiable y profesional, especializado en mantenimiento industrial y gesti√≥n de proyectos. Eres capaz de mantener un hilo conversacional. Tu tarea es proporcionar respuestas claras, bien redactadas y √∫tiles, usando √∫nicamente el contexto proporcionado.

        La informaci√≥n proviene de documentos PDF (como informes t√©cnicos y reportes de avance) y hojas Excel (como cronogramas, presupuestos y seguimientos). Estos pueden incluir texto libre, tablas num√©ricas y descripciones detalladas de actividades.

        Tu estilo debe ser t√©cnico pero accesible: escribe en lenguaje formal, redacta oraciones completas, resume si es necesario y evita repetir frases textuales del contexto sin adaptarlas. Siempre responde de forma elegante, clara y precisa.

        Comportamiento esperado:
        1. Si el usuario hace un saludo, agradecimiento u opini√≥n, responde cordialmente y con empat√≠a.
        2. Si la pregunta actual es una continuaci√≥n de una pregunta anterior, aseg√∫rate de mantener el mismo contexto y responde de manera coherente.
        3. Si la respuesta no est√° textual pero se puede deducir, hazlo de forma l√≥gica, sin inventar.
        4. Si no encuentras suficiente informaci√≥n para responder la pregunta, ind√≠calo de forma profesional.
        5. Si el usuario solicita una tabla, primero describe brevemente su contenido y luego muestra la tabla en markdown.


)   
  """
def construir_contexto(fragmentos):
    contexto = ""
    for f in fragmentos:
        texto_fragmento = f.get("texto", "").strip()
        if texto_fragmento:
            document_id = f.get("document_id", "Documento desconocido")
            fragment_id = f.get("fragment_id", "Fragmento sin nombre")
            sheet_name = f.get("sheet_name", "")
            encabezado = f"[document_id: {document_id}] [fragment_id: {fragment_id}]"
            if sheet_name:
                encabezado += f" [sheet_name: {sheet_name}]"
            contexto += encabezado + "\n" + texto_fragmento + "\n\n---\n\n"
    return contexto

def obtener_respuesta(pregunta, historial, contexto):
    mensajes = [{"role": "system", "content": SYSTEM_PROMPT}]
    mensajes.append({"role": "user", "content": f"Contexto documental:\n{contexto}"})
    mensajes += historial[-MAX_MENSAJES_HISTORIAL:]
    mensajes.append({"role": "user", "content": pregunta})
    return chat_completion(mensajes)



def respuesta_contiene_info(respuesta, pregunta, contexto):
    prompt = (
        f"Pregunta: {pregunta}\n"
        f"Contexto: {contexto}\n"
        f"Respuesta: {respuesta}\n"
        "¬øLa respuesta contiene informaci√≥n relevante del contexto? (s√≠/no):"
    )
    mensajes = [
        {"role": "system", "content": "Eres un verificador de relevancia. Responde solo s√≠ o no."},
        {"role": "user", "content": prompt}
    ]
    veredicto = chat_completion(mensajes).strip().lower()
    return veredicto.startswith("s")


'''
def respuesta_contiene_info(respuesta, contexto, pregunta):
    tokens_respuesta = set(respuesta.lower().split())
    tokens_contexto = set(contexto.lower().split())
    tokens_pregunta = set(pregunta.lower().split()) 

    interseccion = tokens_respuesta & tokens_contexto
    score = len(interseccion) / (len(tokens_respuesta) + 1)

    frases_genericas = {
        "no se encontr√≥", "no hay informaci√≥n", "no incluye", "no contiene",
        "no se dispone", "no existe", "no est√° disponible"
    }
    contiene_negacion = any(frase in respuesta.lower() for frase in frases_genericas)

    repite_pregunta = len(tokens_respuesta & tokens_pregunta) / (len(tokens_pregunta) + 1) > 0.5

    # Si la respuesta es corta y contiene palabra clave de contexto o pregunta, ac√©ptala
    if not contiene_negacion and len(respuesta.split()) < 12 and (len(interseccion) > 0 or len(tokens_respuesta & tokens_pregunta) > 0):
        return True

    return score > 0.05 and not contiene_negacion and not repite_pregunta

'''

def reformular_pregunta_explicita_gpt(pregunta, objeto, historial):
    prompt = (
        "Historial de la conversaci√≥n:\n"
    )
    for mensaje in historial[-3:]:
        prompt += f"{mensaje['role']}: {mensaje['content']}\n"
    prompt += (
        f"Pregunta: {pregunta}\n"
        f"Objeto: {objeto}\n"
        "Reformula la pregunta para que sea expl√≠cita y clara, usando el objeto si es necesario. "
        "Aseg√∫rate de que la pregunta resultante sea espec√≠fica y permita obtener una respuesta detallada y t√©cnica, incluyendo datos num√©ricos, modelos, fechas o cualquier informaci√≥n relevante del contexto. "
        "Si la pregunta ya es expl√≠cita, rep√≠tela tal cual."
    )
    mensajes = [
        {"role": "system", "content": "Eres un asistente experto en mantenimiento industrial. Reformula preguntas impl√≠citas de manera expl√≠cita, clara y t√©cnica, usando el contexto conversacional y asegurando que la nueva pregunta permita obtener respuestas detalladas y precisas."},
        {"role": "user", "content": prompt}
    ]
    return chat_completion(mensajes).strip()
def buscar_y_responder(pregunta, historial):
    fragmentos = buscar_fragmentos(pregunta)
    fragmentos = sorted(fragmentos, key=lambda x: x.get("score", 0), reverse=True)[:6] if fragmentos else []
    contexto = construir_contexto(fragmentos)
    if not contexto:
        return "", contexto
    respuesta = obtener_respuesta(pregunta, historial, contexto)
    return respuesta, contexto



''''
def es_cambio_de_tema(historial, pregunta_actual):
    if not historial:
        return False
    pregunta_anterior = ""
    # Busca la √∫ltima pregunta del usuario en el historial
    for mensaje in reversed(historial):
        if mensaje["role"] == "user":
            pregunta_anterior = mensaje["content"]
            break
    if not pregunta_anterior:
        return False
    prompt = (
        "Dadas dos preguntas de un usuario, responde solo 's√≠' si la segunda pregunta trata sobre un tema diferente a la primera, "
        "o 'no' si ambas preguntas tratan sobre el mismo tema o son seguimiento. "
        f"Primera pregunta: {pregunta_anterior}\n"
        f"Segunda pregunta: {pregunta_actual}\n"
        "¬øLa segunda pregunta es de un tema diferente? (s√≠/no):"
    )
    mensajes = [
        {"role": "system", "content": "Eres un detector de cambio de tema. Responde solo s√≠ o no."},
        {"role": "user", "content": prompt}
    ]
    veredicto = chat_completion(mensajes).strip().lower()
    return veredicto.startswith("s")
'''



PRONOMBRES_SEGUIMIENTO = {"son", "es", "fue", "fueron", "tiene", "tienen", "hay", "cu√°nto", "cu√°ntos", "cu√°ntas", "qu√©", "cu√°l", "cu√°les", "d√≥nde", "c√≥mo", "por qu√©", "qui√©n", "qui√©nes", "de", "la", "el", "los", "las", "un", "una", "unos", "unas", "su", "sus", "lo", "le", "les", "se"}

def es_cambio_de_tema(historial, pregunta_actual):
    if not historial:
        return False
    pregunta_anterior = ""
    for mensaje in reversed(historial):
        if mensaje["role"] == "user":
            pregunta_anterior = mensaje["content"]
            break
    if not pregunta_anterior:
        return False

    # --- NUEVA HEUR√çSTICA: Si la pregunta es muy corta o solo contiene palabras gen√©ricas, NO es cambio de tema ---
    palabras_actual = set(pregunta_actual.lower().split())
    if len(palabras_actual) < 5 or palabras_actual.issubset(PRONOMBRES_SEGUIMIENTO):
        return False

    # --- HEUR√çSTICA R√ÅPIDA ---
    set_anterior = set(pregunta_anterior.lower().split())
    interseccion = palabras_actual & set_anterior
    if len(interseccion) / max(1, len(palabras_actual | set_anterior)) < 0.4:
        return True

    ratio = difflib.SequenceMatcher(None, pregunta_anterior, pregunta_actual).ratio()
    if ratio < 0.4:
        return True

    # Si la heur√≠stica no decide, llama a GPT (casos ambiguos)
    prompt = (
       # "Dadas dos preguntas de un usuario, responde solo 's√≠' si la segunda pregunta trata sobre un tema diferente a la primera, "
      #  "o 'no' si ambas preguntas tratan sobre el mismo tema o son seguimiento. "
        f"Primera pregunta: {pregunta_anterior}\n"
        f"Segunda pregunta: {pregunta_actual}\n"
        "¬øLa segunda pregunta es de un tema diferente? (s√≠/no):"
    )
    mensajes = [
        {"role": "system", "content": "Eres un detector de cambio de tema. Responde solo s√≠ o no."},
        {"role": "user", "content": prompt}
    ]
    veredicto = chat_completion(mensajes).strip().lower()
    return veredicto.startswith("s")

def responder_asistente(pregunta: str, user_id: str) -> str:
    try:
        print(f"\nüîç Pregunta recibida de {user_id}: {pregunta}")
        
        # Inicializar historial si es la primera vez del usuario
        if user_id not in HISTORIAL_CONVERSACION:
            HISTORIAL_CONVERSACION[user_id] = []

        historial = HISTORIAL_CONVERSACION[user_id]

        # Buscar nuevos fragmentos relacionados con la pregunta
        print("üîé Buscando nuevos fragmentos...")
        fragmentos_relevantes = buscar_fragmentos(pregunta)

        if not fragmentos_relevantes:
            print("‚ö†Ô∏è No se encontraron fragmentos relevantes.")
            respuesta = "No se encontr√≥ informaci√≥n relevante en los documentos para responder tu pregunta."
            historial.append({"role": "user", "content": pregunta})
            historial.append({"role": "assistant", "content": respuesta})
            HISTORIAL_CONVERSACION[user_id] = historial[-MAX_MENSAJES_HISTORIAL:]
            return respuesta

        # Tomar los 6 mejores fragmentos
        fragmentos_relevantes = sorted(fragmentos_relevantes, key=lambda x: x.get("score", 0), reverse=True)[:6]

        # Construir el contexto de los fragmentos encontrados
        contexto = ""
        for f in fragmentos_relevantes:
            texto_fragmento = f.get("texto", "").strip()
            if texto_fragmento:
                document_id = f.get("document_id", "Documento desconocido")
                fragment_id = f.get("fragment_id", "Fragmento sin nombre")
                sheet_name = f.get("sheet_name", "")

                encabezado = f"[document_id: {document_id}] [fragment_id: {fragment_id}]"
                if sheet_name:
                    encabezado += f" [sheet_name: {sheet_name}]"

                contexto += encabezado + "\n" + texto_fragmento + "\n\n---\n\n"

        if not contexto.strip():
            print("‚ö†Ô∏è Fragmentos sin contenido √∫til.")
            respuesta = "Encontr√© documentos, pero no pude acceder al contenido."
            historial.append({"role": "user", "content": pregunta})
            historial.append({"role": "assistant", "content": respuesta})
            HISTORIAL_CONVERSACION[user_id] = historial[-MAX_MENSAJES_HISTORIAL:]
            return respuesta

        # Construir la lista de mensajes para GPT
        mensajes = [{"role": "system", "content": SYSTEM_PROMPT}]
        mensajes.append({"role": "user", "content": f"Contexto documental:\n{contexto}"})
        mensajes += historial[-MAX_MENSAJES_HISTORIAL:]
        mensajes.append({"role": "user", "content": pregunta})

        print("ü§ñ Enviando a GPT...")
        respuesta = chat_completion(mensajes)
        print("‚úÖ Respuesta generada")

        # Guardar la pregunta y la respuesta en el historial
        historial.append({"role": "user", "content": pregunta})
        historial.append({"role": "assistant", "content": respuesta})
        HISTORIAL_CONVERSACION[user_id] = historial[-MAX_MENSAJES_HISTORIAL:]

        return respuesta

    except Exception as e:
        print(f"‚ùå Error: {str(e)}")
        return "Lo siento, ocurri√≥ un error interno al procesar tu pregunta."
